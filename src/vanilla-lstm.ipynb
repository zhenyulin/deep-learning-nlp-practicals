{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forget_gate(x, h, W_fh, B_fh, W_fx, B_fx, previous_cell_state):\n",
    "    '''\n",
    "    Forget Gate is a way to selectively forget some of what the Cell State(LTM) has in memory.\n",
    "    \n",
    "    The New Event and the previous timestep's Hidden State are summed and then transformed via sigmoid\n",
    "    to output a probability vector of values in (0,1) to proportionally retrieve states in LTM.\n",
    "    \n",
    "    cell_state records LTM and will be transformed to produce new_hidden_state(Output, STM) via element-wise product\n",
    "    therefore, cell_state(LTM) and hidden_state(STM) will be of the same shape [hidden_size, 1]\n",
    "    \n",
    "    forget gate is designed to produce a probability matrix taking effect on cell_state via element-wise product\n",
    "    therefore output of the forget gate is of the same shape as cell_state [hidden_size, 1]\n",
    "    '''\n",
    "    # retrospectively, to transform input to the probability candidates of the output shape\n",
    "    # W_fx is [hidden_size, input_size]\n",
    "    # B_fx is [hidden_size, 1]\n",
    "    forget_eventx = W_fx @ x + B_fx\n",
    "    # W_fh is [hidden_size, hidden_size] to transform previous hidden state\n",
    "    # B_fh is [hidden_size, 1]\n",
    "    forget_hidden = W_fh @ h + B_fh\n",
    "    # information(candidates transformed from inputs/features and previous hidden states/candidates)\n",
    "    # is combined here via addition, in fact the matrix in the computation can be concatted\n",
    "    # so that if there's additional feature information from the input, the candidates from the STM can be enhanced\n",
    "    # to decide what to retrieve from the LTM\n",
    "    combined = forget_hidden + forget_eventx\n",
    "    # then the combined (via addition) candidates are normalised to (0, 1) via sigmoid\n",
    "    # to be used as probability to forget/use candidates stored in previous cell state\n",
    "    squarshed = sigmoid(combined)\n",
    "    # use an element-wise multiplication to forget/reduce the candidates stored in previous cell state\n",
    "    selected_cell_state = np.multiply(squarshed, previous_cell_state)\n",
    "    return selected_cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_gate(x, h, W_ih, B_ih, W_ix, B_ix, W_lh, B_lh, W_lx, B_lx):\n",
    "    '''\n",
    "    The Input Gate has 2 components: \n",
    "    - the ignore matrix ([hidden_size, 1] probability matrix)\n",
    "    - the learnt matrix([hidden_size, 1] feature matrix of values(-1, 1))\n",
    "    -> They are combined use element-wise product to selectively put values(feature weights) in the learnt matrix.\n",
    "    \n",
    "    - the ignore matrix is produced via combined transformation of input and previoius hidden state, output with sigmoid\n",
    "      - similar to forget gate\n",
    "    - the learnt matrix is produced via combined transformation of input and previous hidden state, output with tanh\n",
    "      - tanh output (-1, 1) fits feature matrix to learn both positive/negtive relationships from input data\n",
    "    \n",
    "    the effect is only parts of the newly learnt information(features) is put through to be added \n",
    "    with selected cell_state from forget gate to update LTM as new cell state\n",
    "    '''\n",
    "    ignore_hidden = w_ih @ h + b_ih\n",
    "    ignore_eventx = w_ix @ x + b_ix\n",
    "    learn_hidden = w_lh @ h + b_lh\n",
    "    learn_eventx = w_lx @ h + b_lx\n",
    "    ignore_matrix = sigmoid(ignore_hidden + ignore_eventx)\n",
    "    learn_matrix = tanh(learn_hidden + learn_eventx)\n",
    "    selected_learn = np.multiply(ignore_matrix, learn_matrix)\n",
    "    return selected_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_state(forget_gate_output, input_gate_output):\n",
    "    return forget_gate_output + input_gate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_gate(x, h, w_oh, b_ob, w_ox, b_ox, cell_state):\n",
    "    \"\"\"\n",
    "    Output Gate output the carry-forward STM (the new hidden state), a.k.a. the prediction output from the event.\n",
    "    \n",
    "    - the new event and previous hidden state (STM) are combined to produce a selection matrix (probability)\n",
    "    - to selectively activate/retrieve features/weights stored in cell_state(LTM) \n",
    "    \"\"\"\n",
    "    out_hidden = w_oh @ h + b_oh\n",
    "    out_eventx = w_ox @ x + b_ox\n",
    "    out_matrix = sigmoid(out_hidden + out_eventx)\n",
    "    selected_ltm = np.multiply(out_matrix, cell_state)\n",
    "    return selected_ltm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Wfh = randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wih = randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wlh = randn(hidden_size, hidden_size) / 1000\n",
    "        self.Woh = randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wfx = randn(hidden_size, input_size) / 1000\n",
    "        self.Wix = randn(hidden_size, input_size) / 1000\n",
    "        self.Wlx = randn(hidden_size, input_size) / 1000\n",
    "        self.Wox = randn(hidden_size, input_size) / 1000\n",
    "\n",
    "        self.Bfh = np.zeros((hidden_size, 1))\n",
    "        self.Bih = np.zeros((hidden_size, 1))\n",
    "        self.Blh = np.zeros((hidden_size, 1))\n",
    "        self.Boh = np.zeros((hidden_size, 1))\n",
    "        self.Bfx = np.zeros((hidden_size, 1))\n",
    "        self.Bix = np.zeros((hidden_size, 1))\n",
    "        self.Blx = np.zeros((hidden_size, 1))\n",
    "        self.Box = np.zeros((hidden_size, 1))\n",
    "\n",
    "        self.Wyh = randn(hidden_size, input_size) / 1000\n",
    "        self.Byh = np.zeros((hidden_size, 1))\n",
    "\n",
    "    def Cell(self, previous_cell_state, previous_hidden_state, event_x):\n",
    "        h = previous_hidden_state\n",
    "        x = event_x\n",
    "\n",
    "        w_fh = self.Wfh\n",
    "        w_ih = self.Wih\n",
    "        w_lh = self.Wlh\n",
    "        w_oh = self.Woh\n",
    "        w_fx = self.Wfx\n",
    "        w_ix = self.Wix\n",
    "        w_lx = self.Wlx\n",
    "        w_ox = self.Wox\n",
    "\n",
    "        b_fh = self.Bfh\n",
    "        b_ih = self.Bih\n",
    "        b_lh = self.Blh\n",
    "        b_oh = self.Boh\n",
    "        b_fx = self.Bfx\n",
    "        b_ix = self.Bix\n",
    "        b_lx = self.Blx\n",
    "        b_ox = self.Box\n",
    "\n",
    "        selected_cell_state = forget_gate(x, h, w_fh, b_fh, w_fx, b_fx)\n",
    "        selected_learnt_input = input_gate(\n",
    "            x, h, w_ih, b_ih, w_ix, b_ix, w_lh, b_lh, w_lx, b_lx\n",
    "        )\n",
    "        new_cell_state = cell_state(selected_cell_state, selected_learnt_input)\n",
    "        new_hidden_state = output_gate(\n",
    "            x, h, w_oh, b_oh, w_ox, b_ox, previous_cell_state\n",
    "        )\n",
    "\n",
    "        return new_hidden_state, new_cell_state\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        for i, x in enumerate(inputs):\n",
    "            h, c = self.Cell(self, c, h, x)\n",
    "\n",
    "        y = self.Wyh @ h + self.Byh\n",
    "\n",
    "        return y, h, c\n",
    "    \n",
    "    def backprop(self):\n",
    "        '''\n",
    "        not implemented here as the derivative calculation can be lengthy\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In summary, there are two types of weights and biases of 4 sets\n",
    "- weights/biases to transform previous_hidden_state of [hidden_size, hidden_size]  [hidden_size, 1]\n",
    "- weights/biases to transform input [hidden_size, input_size], [hidden_size, 1]\n",
    "\n",
    "Each set of weights have 4 components in 3 different gates, all performaing 'Wgh @ h + Bgh' or 'Wgx @ x + Bgx'\n",
    "if combined in computation forming a [4*hidden_size, hidden_size] or [4*hidden_size, input_size] matrix \n",
    "output a [4*hidden_size, 1] matrix which can be then further splitted back to 4 [hidden_size, 1] matrix\n",
    "\n",
    "In fact the two sets of weights can also be concatted in computation to form [4*hidden_size, hidden_size + input_size + 1] matrix\n",
    "product with [hidden_size+input_size+1, 1], output a [4*hidden_size, 1] matrix\n",
    "which can be cut into 4 matrix and fed into sigmoid/tanh to produce the neuron output \n",
    "\n",
    "p.s. let's say neuron is part of a cell here\n",
    "in fact it would be better to call each nn components \"knots\" (combination of two neural nets)\n",
    "to rename 'cell' 'pod' (group of cells/neurons/knots with a specific internal structure)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference\n",
    "#\n",
    "# https://towardsdatascience.com/the-lstm-reference-card-6163ca98ae87\n",
    "# https://blog.varunajayasiri.com/numpy_lstm.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
