{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "from data.sentiment_dataset import train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set([word for text in train_data.keys() for word in text.split(' ')]))\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "was\n"
     ]
    }
   ],
   "source": [
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx_to_word = {i: w for i, w in enumerate(vocab)}\n",
    "print(word_to_idx['good'])\n",
    "print(idx_to_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createInputs(text):\n",
    "    '''\n",
    "    Returns a list of one-hot vectors representing the words \n",
    "    in the input text string.\n",
    "    - @param {string} text\n",
    "    - @returns {list} one-hot vector with a shape of (vocab_size, 1)\n",
    "    '''\n",
    "    inputs = []\n",
    "    for word in text.split(' '):\n",
    "        v = np.zeros((vocab_size, 1))\n",
    "        v[word_to_idx[word]] = 1\n",
    "        inputs.append(v)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # A Vanilla Recurrent Neural Network\n",
    "    # ht = tanh(WxhXt + WhhHt-1 + Bh)\n",
    "    # yt = WhyHt + By\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.hidden_size = hidden_size\n",
    "        self.Whh = randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = randn(hidden_size, input_size) / 1000\n",
    "        self.Why = randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Bias\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of the RNN using the given inputs.\n",
    "        Returns the final output and hidden state.\n",
    "        - @param {list} inputs - list of one-hot vectors with shape (input_size, 1)\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "        self.inputs = inputs\n",
    "        self.hs = {0: h}\n",
    "\n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.hs[i + 1] = h\n",
    "\n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backprop(self, dy, learn_rate=2e-2):\n",
    "        \"\"\"\n",
    "        Perform a back propogation to update the weights\n",
    "        - @param{list} dy the gradient (dL/dy) received for each output with a shape of (output_size, 1)\n",
    "        - @param{float} learn_rate\n",
    "        \"\"\"\n",
    "        n = len(self.inputs)\n",
    "\n",
    "        # calculate dL/dWhy and dL/dby from the linear output\n",
    "        d_Why = dy @ self.hs[n].T\n",
    "        d_by = dy\n",
    "\n",
    "        # Initialize dL/dWhh, dL/dWxh, dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # calculate dL/dh for the last h.\n",
    "        d_h = self.Why.T @ dy\n",
    "\n",
    "        for t in reversed(range(n)):\n",
    "            # An intermediate value: dL/dh * (1 - h^2)\n",
    "            temp = (1 - self.hs[t + 1] ** 2) * d_h\n",
    "\n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += temp\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += temp @ self.hs[t].T\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            d_Wxh += temp @ self.inputs[t].T\n",
    "            # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = self.Whh @ temp\n",
    "\n",
    "        # Clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update the weights and biases using gradent descent\n",
    "        self.Whh -= learn_rate * d_Whh\n",
    "        self.Wxh -= learn_rate * d_Wxh\n",
    "        self.Why -= learn_rate * d_Why\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RNN\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "inputs = createInputs('i am very good')\n",
    "out, h = rnn.forward(inputs)\n",
    "probs = softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.50000387],\n",
      "       [0.49999613]])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def processData(data, backprop=True):\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for x, y in items:\n",
    "        inputs = createInputs(x)\n",
    "        target = int(y)\n",
    "\n",
    "        # forward\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        probs = softmax(out)\n",
    "\n",
    "        # calculate loss / accuracy\n",
    "        loss -= np.log(probs[target])[0]\n",
    "        num_correct += int(np.argmax(probs) == target)\n",
    "\n",
    "        if backprop:\n",
    "            # build dL/dy\n",
    "            # dL/dy = pi - y = pi - 1 if i==c else pi\n",
    "            dl_dy = probs\n",
    "            dl_dy[target] -= 1\n",
    "\n",
    "            rnn.backprop(dl_dy)\n",
    "\n",
    "    return loss / len(data), num_correct / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 100\n",
      "Train Loss: 0.002 | Accuracy: 1.000\n",
      "Test Loss: 0.568 | Accuracy: 0.950\n",
      "-- Epoch 200\n",
      "Train Loss: 0.002 | Accuracy: 1.000\n",
      "Test Loss: 0.580 | Accuracy: 0.950\n",
      "-- Epoch 300\n",
      "Train Loss: 0.001 | Accuracy: 1.000\n",
      "Test Loss: 0.589 | Accuracy: 0.950\n",
      "-- Epoch 400\n",
      "Train Loss: 0.001 | Accuracy: 1.000\n",
      "Test Loss: 0.598 | Accuracy: 0.950\n",
      "-- Epoch 500\n",
      "Train Loss: 0.001 | Accuracy: 1.000\n",
      "Test Loss: 0.605 | Accuracy: 0.950\n",
      "-- Epoch 600\n",
      "Train Loss: 0.001 | Accuracy: 1.000\n",
      "Test Loss: 0.612 | Accuracy: 0.950\n",
      "-- Epoch 700\n",
      "Train Loss: 0.001 | Accuracy: 1.000\n",
      "Test Loss: 0.619 | Accuracy: 0.950\n",
      "-- Epoch 800\n",
      "Train Loss: 0.001 | Accuracy: 1.000\n",
      "Test Loss: 0.625 | Accuracy: 0.950\n",
      "-- Epoch 900\n",
      "Train Loss: 0.001 | Accuracy: 1.000\n",
      "Test Loss: 0.631 | Accuracy: 0.950\n",
      "-- Epoch 1000\n",
      "Train Loss: 0.001 | Accuracy: 1.000\n",
      "Test Loss: 0.636 | Accuracy: 0.950\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    train_loss, train_acc = processData(train_data)\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print(f\"-- Epoch {epoch + 1}\")\n",
    "        print(f\"Train Loss: {train_loss:.3f} | Accuracy: {train_acc:.3f}\")\n",
    "\n",
    "        test_loss, test_acc = processData(test_data, backprop=False)\n",
    "        print(f\"Test Loss: {test_loss:.3f} | Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.0025818],\n",
      "       [0.9974182]])\n"
     ]
    }
   ],
   "source": [
    "inputs = createInputs('i am very good')\n",
    "out, h = rnn.forward(inputs)\n",
    "probs = softmax(out)\n",
    "pprint(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
